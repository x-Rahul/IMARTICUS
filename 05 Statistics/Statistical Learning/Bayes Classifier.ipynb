{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44d9a20e-98a1-49ac-9c47-400fd3bddc1c",
   "metadata": {},
   "source": [
    "### **Bayes' Theorem:**\n",
    "\n",
    "Bayes' Theorem provides a way to update our beliefs about the probability of a hypothesis \\( H \\) (such as a class label) given new evidence \\( E \\) (such as a feature vector). Mathematically, it is expressed as:\n",
    "\n",
    "$$\n",
    "P(H | E) = \\frac{P(E | H) \\cdot P(H)}{P(E)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $P(H | E)$ is the **posterior probability**: the probability of the hypothesis $H$ given the evidence $E$.\n",
    "- $P(E | H)$ is the **likelihood**: the probability of the evidence $E$ given that the hypothesis $H$ is true.\n",
    "- $P(H)$ is the **prior probability**: the initial probability of the hypothesis $H$ before seeing the evidence.\n",
    "- $P(E)$ is the **marginal likelihood** or **evidence**: the total probability of the evidence under all possible hypotheses.\n",
    "\n",
    "### **Bayes Classifier:**\n",
    "\n",
    "In the context of classification, the Bayes classifier uses Bayes' Theorem to determine the probability that an observation $\\mathbf{x}$ belongs to a particular class $C_k$. The goal is to assign the observation to the class with the highest posterior probability.\n",
    "\n",
    "### **Mathematical Formulation:**\n",
    "\n",
    "Given a set of classes $C_1, C_2, \\dots, C_K$, and an observation $\\mathbf{x}$, the Bayes classifier assigns $\\mathbf{x}$ to the class $C_k$ that maximizes the posterior probability:\n",
    "\n",
    "$$\n",
    "\\hat{C}(\\mathbf{x}) = \\underset{k \\in \\{1, 2, \\dots, K\\}}{\\arg\\max} \\, P(C_k | \\mathbf{x})\n",
    "$$\n",
    "\n",
    "Using Bayes' Theorem, this can be rewritten as:\n",
    "\n",
    "$$\n",
    "\\hat{C}(\\mathbf{x}) = \\underset{k \\in \\{1, 2, \\dots, K\\}}{\\arg\\max} \\, \\frac{P(\\mathbf{x} | C_k) \\cdot P(C_k)}{P(\\mathbf{x})}\n",
    "$$\n",
    "\n",
    "Since $P(\\mathbf{x})$ is the same for all classes, it does not affect the maximization, so we can simplify the decision rule to:\n",
    "\n",
    "$$\n",
    "\\hat{C}(\\mathbf{x}) = \\underset{k \\in \\{1, 2, \\dots, K\\}}{\\arg\\max} \\, P(\\mathbf{x} | C_k) \\cdot P(C_k)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $P(\\mathbf{x} | C_k)$ is the likelihood: the probability of observing $\\mathbf{x}$ given that the true class is $C_k$.\n",
    "- $P(C_k)$ is the prior probability: the probability that a randomly chosen observation belongs to class $C_k$.\n",
    "\n",
    "### **Interpretation:**\n",
    "\n",
    "- **Prior $P(C_k)$:** Represents our initial belief about the frequency or probability of each class. For example, if we know that 70% of emails are not spam and 30% are spam, then $P(\\text{Not Spam}) = 0.7$ and $P(\\text{Spam}) = 0.3$.\n",
    "- **Likelihood $P(\\mathbf{x} | C_k)$:** Reflects how likely it is to observe the features $\\mathbf{x}$ given that the observation belongs to class $C_k$. For example, the likelihood of certain words appearing in a spam email might be higher than in a non-spam email.\n",
    "\n",
    "### **Gaussian Naive Bayes Example:**\n",
    "\n",
    "In practice, a common implementation is the Gaussian Naive Bayes classifier, where we assume that the likelihood of the features $\\mathbf{x}$ given the class $C_k$ follows a Gaussian (normal) distribution:\n",
    "\n",
    "$$\n",
    "P(x_j | C_k) = \\frac{1}{\\sqrt{2 \\pi \\sigma_k^2}} \\exp \\left( -\\frac{(x_j - \\mu_{k})^2}{2 \\sigma_k^2} \\right)\n",
    "$$\n",
    "\n",
    "Where $\\mu_k$ and $\\sigma_k^2$ are the mean and variance of the feature $x_j$ for class $C_k$.\n",
    "\n",
    "### **Final Decision Rule:**\n",
    "\n",
    "The final decision rule combines the prior and the likelihood:\n",
    "\n",
    "$$\n",
    "\\hat{C}(\\mathbf{x}) = \\underset{k \\in \\{1, 2, \\dots, K\\}}{\\arg\\max} \\, \\log(P(C_k)) + \\sum_{j=1}^{p} \\log \\left( P(x_j | C_k) \\right)\n",
    "$$\n",
    "\n",
    "Here, we take the logarithm to avoid underflow issues and to simplify the multiplication of probabilities into a sum.\n",
    "\n",
    "### **Summary:**\n",
    "\n",
    "The Bayes classifier is a powerful probabilistic approach that leverages Bayes' Theorem to classify observations. By combining prior knowledge of class probabilities with the likelihood of observing certain features given those classes, it assigns the observation to the most probable class. Despite its simplicity, it can be very effective, especially when the assumptions (such as feature independence in Naive Bayes) hold true.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f387dd79-6631-48fd-8296-fd65eabb1fd7",
   "metadata": {},
   "source": [
    "### Example: Disease Diagnosis\n",
    "\n",
    "In this example, we'll use a Bayes classifier to determine if a patient has a certain disease based on the presence of symptoms.\n",
    "\n",
    "**Scenario:**\n",
    "\n",
    "We have a dataset with the following features:\n",
    "- **x1:** Presence of Symptom A (1 if present, 0 if not)\n",
    "- **x2:** Presence of Symptom B (1 if present, 0 if not)\n",
    "\n",
    "We want to classify the patient into:\n",
    "- C1: No Disease\n",
    "- C2: Disease\n",
    "\n",
    "**Step 1: Calculate Prior Probabilities**\n",
    "\n",
    "From the training data, we determine:\n",
    "- $P(\\text{No Disease}) = 0.8$\n",
    "- $P(\\text{Disease}) = 0.2$\n",
    "\n",
    "**Step 2: Calculate Likelihoods**\n",
    "\n",
    "We estimate the likelihoods from the data:\n",
    "- $P(x1 = 1 \\mid \\text{No Disease}) = 0.2$\n",
    "- $P(x1 = 1 \\mid \\text{Disease}) = 0.7$\n",
    "- $P(x2 = 1 \\mid \\text{No Disease}) = 0.1$\n",
    "- $P(x2 = 1 \\mid \\text{Disease}) = 0.6$\n",
    "\n",
    "**Step 3: Compute Posterior Probabilities**\n",
    "\n",
    "Consider a new patient with the following symptoms:\n",
    "- $x1 = 1$ (has Symptom A)\n",
    "- $x2 = 1$ (has Symptom B)\n",
    "\n",
    "We need to compute the posterior probabilities for both classes:\n",
    "\n",
    "**For No Disease:**\n",
    "\n",
    "Compute the joint probability:\n",
    "$$\n",
    "P(x \\mid \\text{No Disease}) = P(x1 = 1 \\mid \\text{No Disease}) \\cdot P(x2 = 1 \\mid \\text{No Disease})\n",
    "$$\n",
    "\n",
    "Calculate the posterior probability:\n",
    "$$\n",
    "P(\\text{No Disease} \\mid x) \\propto P(x \\mid \\text{No Disease}) \\cdot P(\\text{No Disease})\n",
    "$$\n",
    "\n",
    "**For Disease:**\n",
    "\n",
    "Compute the joint probability:\n",
    "$$\n",
    "P(x \\mid \\text{Disease}) = P(x1 = 1 \\mid \\text{Disease}) \\cdot P(x2 = 1 \\mid \\text{Disease})\n",
    "$$\n",
    "\n",
    "Calculate the posterior probability:\n",
    "$$\n",
    "P(\\text{Disease} \\mid x) \\propto P(x \\mid \\text{Disease}) \\cdot P(\\text{Disease})\n",
    "$$\n",
    "\n",
    "**Example Calculation:**\n",
    "\n",
    "Letâ€™s do a quick calculation using hypothetical numbers.\n",
    "\n",
    "For a patient with:\n",
    "- $x1 = 1$\n",
    "- $x2 = 1$\n",
    "\n",
    "**For No Disease:**\n",
    "\n",
    "- $P(x1 = 1 \\mid \\text{No Disease}) = 0.2$\n",
    "- $P(x2 = 1 \\mid \\text{No Disease}) = 0.1$\n",
    "\n",
    "$$\n",
    "P(x \\mid \\text{No Disease}) = 0.2 \\cdot 0.1 = 0.02\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\text{No Disease} \\mid x) \\propto 0.02 \\cdot 0.8 = 0.016\n",
    "$$\n",
    "\n",
    "**For Disease:**\n",
    "\n",
    "- $P(x1 = 1 \\mid \\text{Disease}) = 0.7$\n",
    "- $P(x2 = 1 \\mid \\text{Disease}) = 0.6$\n",
    "\n",
    "$$\n",
    "P(x \\mid \\text{Disease}) = 0.7 \\cdot 0.6 = 0.42\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\text{Disease} \\mid x) \\propto 0.42 \\cdot 0.2 = 0.084\n",
    "$$\n",
    "\n",
    "Since $0.084$ (Disease) $>$ $0.016$ (No Disease), we classify the patient as having the Disease.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a739b7-e3aa-491d-aa68-928bd977a657",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
